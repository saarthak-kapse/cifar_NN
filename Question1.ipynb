{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Data \n",
    "    \n",
    "    1) Merging batch_1, batch_2, batch_3, batch_4 and batch_5 data and their labels. Making (50000,3073) data matrix.\n",
    "       Where first 3072 columns corresponds to image pixels and last column belongs to label. And train data is of size 50000.\n",
    "    2)Then from batch_test its data and labels are merged and data matrix of size (10000,3073) is formed.\n",
    "    3)Then cifar_2_data is subset of cifar_10 data which contains data which correspond to label 0 and 1 only.\n",
    "    4)Then cifar_5_data is subset of cifar_10 data which contains data which correspond to label 0,1,2,3,4 only.\n",
    "    5)Same is done for cifar_2_test and cifar_5_test, but with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "batch_1 = unpickle('./image_data/cifar-10-batches-py/data_batch_1')\n",
    "batch_2 = unpickle('./image_data/cifar-10-batches-py/data_batch_2')\n",
    "batch_3 = unpickle('./image_data/cifar-10-batches-py/data_batch_3')\n",
    "batch_4 = unpickle('./image_data/cifar-10-batches-py/data_batch_4')\n",
    "batch_5 = unpickle('./image_data/cifar-10-batches-py/data_batch_5')\n",
    "batch_test = unpickle('./image_data/cifar-10-batches-py/test_batch')\n",
    "\n",
    "data_temp = np.concatenate([batch_1[b'data'],batch_2[b'data'],batch_3[b'data'],batch_1[b'data'],batch_5[b'data']],axis=0)\n",
    "filename_temp = np.concatenate([batch_1[b'filenames'],batch_2[b'filenames'],batch_3[b'filenames'],batch_1[b'filenames'],batch_5[b'filenames']],axis=0)\n",
    "label_temp = np.concatenate([batch_1[b'labels'],batch_2[b'labels'],batch_3[b'labels'],batch_1[b'labels'],batch_5[b'labels']],axis=0)\n",
    "cifar_10_data = np.concatenate((data_temp,label_temp.reshape((-1,1))),axis=1)\n",
    "cifar_10_data_test = np.concatenate([batch_test[b'data'],np.array(batch_test[b'labels']).reshape((-1,1))],axis=1)\n",
    "\n",
    "cifar_2_data = cifar_10_data[cifar_10_data[:,-1]<= 1]                      \n",
    "cifar_2_data_test = cifar_10_data_test[cifar_10_data_test[:,-1]<= 1]\n",
    "cifar_5_data = cifar_10_data[cifar_10_data[:,-1]<= 4]\n",
    "cifar_5_data_test = cifar_10_data_test[cifar_10_data_test[:,-1]<= 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating mean and std\n",
    "    1) Mean of each channel is calculated from train data for cifar_2_data and cifar_5_data seperately.\n",
    "    2) Similarly standard deviation of each channel is calculated from train data for cifar_2_data and cifar_5_data seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_2_data = cifar_2_data.astype(float)\n",
    "cifar_2_data_test = cifar_2_data_test.astype(float)\n",
    "cifar_5_data = cifar_5_data.astype(float)\n",
    "cifar_5_data_test = cifar_5_data_test.astype(float)\n",
    "\n",
    "r_mean = np.mean(cifar_2_data[:1024,:-1])\n",
    "g_mean = np.mean(cifar_2_data[1024:2048,:-1])\n",
    "b_mean = np.mean(cifar_2_data[2048:3072,:-1])\n",
    "r_std = np.std(cifar_2_data[:1024,:-1])\n",
    "g_std = np.std(cifar_2_data[1024:2048,:-1])\n",
    "b_std = np.std(cifar_2_data[2048:3072,:-1])\n",
    "\n",
    "r_mean_5 = np.mean(cifar_5_data[:1024,:-1])\n",
    "g_mean_5 = np.mean(cifar_5_data[1024:2048,:-1])\n",
    "b_mean_5 = np.mean(cifar_5_data[2048:3072,:-1])\n",
    "r_std_5 = np.std(cifar_5_data[:1024,:-1])\n",
    "g_std_5 = np.std(cifar_5_data[1024:2048,:-1])\n",
    "b_std_5 = np.std(cifar_5_data[2048:3072,:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Data\n",
    "    1) Train Data and test data for both cifar_2 and cifar_5 is normalized by subtracting mean calculated above and diving it by standard deviation of each channel seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_2_data[:1024,:-1] = (cifar_2_data[:1024,:-1] - r_mean)/r_std\n",
    "cifar_2_data[1024:2048,:-1] = (cifar_2_data[1024:2048,:-1] - g_mean)/g_std\n",
    "cifar_2_data[2048:3072,:-1] = (cifar_2_data[2048:3072,:-1] - b_mean)/b_std\n",
    "\n",
    "cifar_2_data_test[:1024,:-1] = (cifar_2_data_test[:1024,:-1] - r_mean)/r_std\n",
    "cifar_2_data_test[1024:2048,:-1] = (cifar_2_data_test[1024:2048,:-1] - g_mean)/g_std\n",
    "cifar_2_data_test[2048:3072,:-1] = (cifar_2_data_test[2048:3072,:-1] - b_mean)/b_std\n",
    "\n",
    "cifar_5_data[:1024,:-1] = (cifar_5_data[:1024,:-1] - r_mean_5)/r_std_5\n",
    "cifar_5_data[1024:2048,:-1] = (cifar_5_data[1024:2048,:-1] - g_mean_5)/g_std_5\n",
    "cifar_5_data[2048:3072,:-1] = (cifar_5_data[2048:3072,:-1] - b_mean_5)/b_std_5\n",
    "\n",
    "cifar_5_data_test[:1024,:-1] = (cifar_5_data_test[:1024,:-1] - r_mean_5)/r_std_5\n",
    "cifar_5_data_test[1024:2048,:-1] = (cifar_5_data_test[1024:2048,:-1] - g_mean_5)/g_std_5\n",
    "cifar_5_data_test[2048:3072,:-1] = (cifar_5_data_test[2048:3072,:-1] - b_mean_5)/b_std_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input a set of training examples\n",
    "    For each training example x: Set the corresponding input activation ax,1, and perform the following steps:\n",
    "    Feedforward: For each l=2,3,…,L compute zx,l=wlax,l−1+bl and ax,l=σ(zx,l).\n",
    "    Output error δx,L: Compute the vector δx,L=∇aCx⊙σ′(zx,L).\n",
    "    Backpropagate the error: For each l=L−1,L−2,…,2 compute δx,l=((wl+1)Tδx,l+1)⊙σ′(zx,l).\n",
    "    Gradient descent: For each l=L,L−1,…,2 update the weights according to the rule wl→wl−ηm∑xδx,l(ax,l−1)T, and the biases \n",
    "    according to the rule bl→bl−ηm∑xδx,l.\n",
    "    reference :- http://neuralnetworksanddeeplearning.com/chap2.html\n",
    "                 https://doug919.github.io/notes-on-backpropagation-with-cross-entropy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network():\n",
    "    \n",
    "    def __init__(self,architecture,pretrained=False,model_path=None):\n",
    "        \n",
    "        if pretrained is False:\n",
    "            self.bias_matrix = [np.random.randn(number_of_neuron)/math.sqrt(3072) for number_of_neuron in architecture[1:]]\n",
    "            self.weights_matrix = [(np.random.randn(neuron_in_next_layers, neuron_in_previous_layers)/math.sqrt(3072))\\\n",
    "                                   for neuron_in_previous_layers, neuron_in_next_layers in zip(architecture[:-1], architecture[1:])]\n",
    "        else:\n",
    "            with open(model_path+'model_weight.pkl','rb') as f:\n",
    "                self.weights_matrix = pickle.load(f)\n",
    "            with open(model_path+'model_bais.pkl','rb') as b:\n",
    "                self.bias_matrix = pickle.load(b)\n",
    "            \n",
    "        self.num_layers = len(architecture)\n",
    "        self.architecture = architecture\n",
    "    \n",
    "    def fc_layer(self,input_):\n",
    "        temp = input_.copy()\n",
    "        count = 0 \n",
    "        for bais,weights in zip(self.bias_matrix,self.weights_matrix):\n",
    "            if count<len(bais)-1:\n",
    "                temp = self.relu(np.matmul(weights,temp)+bais)\n",
    "            else:\n",
    "                temp = self.softmax(np.matmul(weights,temp)+bais)\n",
    "            count += 1\n",
    "        return temp \n",
    "    \n",
    "    def relu(self,vector):\n",
    "        temp = vector.copy()\n",
    "        return np.where(temp>=0,temp,0.0)\n",
    "    \n",
    "    def softmax(self,vector):\n",
    "        temp = vector.copy()\n",
    "        temp = np.exp(temp)\n",
    "        return temp/np.sum(temp)\n",
    "    \n",
    "    def cross_entropy(self,vector,label):\n",
    "        temp1 = vector.copy()\n",
    "        temp2 = np.zeros(temp1.shape[0])\n",
    "        temp2[int(label)] = 1\n",
    "        temp1 = np.clip(temp1,1e-2,1-(1e-2))\n",
    "        return -1*np.sum(np.log(temp1)*temp2)/temp1.shape[0]\n",
    "    \n",
    "    def backpropagation(self, image, gt_truth):\n",
    "        image_delta_w = [np.zeros((neuron_in_next_layers, neuron_in_previous_layers))\\\n",
    "                               for neuron_in_previous_layers, neuron_in_next_layers in zip(self.architecture[:-1], self.architecture[1:])]\n",
    "        image_delta_b = [np.zeros(number_of_neuron) for number_of_neuron in self.architecture[1:]]\n",
    "        \n",
    "        temp = image.copy()\n",
    "        z_layers=[]\n",
    "        activation_layers=[]\n",
    "        activation_layers.append(temp)\n",
    "        count = 0 \n",
    "        for bais,weights in zip(self.bias_matrix,self.weights_matrix):\n",
    "            temp = np.matmul(weights,temp)+bais\n",
    "            z_layers.append(temp)\n",
    "            if count<len(bais)-1:\n",
    "                temp = self.relu(temp)\n",
    "            else:\n",
    "                temp = self.softmax(temp)\n",
    "                \n",
    "            activation_layers.append(temp)\n",
    "            count += 1\n",
    "        \n",
    "        one_hot_vector_label = np.zeros(temp.shape[0])\n",
    "        one_hot_vector_label[int(gt_truth)] = 1\n",
    "        del_cost_activation = temp - one_hot_vector_label\n",
    "        image_delta_w[-1] = np.matmul(del_cost_activation.reshape((-1,1)), activation_layers[-2].reshape((1,-1)))\n",
    "        image_delta_b[-1] = del_cost_activation.copy()\n",
    "    \n",
    "        \n",
    "        for i in range(2,len(image_delta_w)+1):\n",
    "            del_cost_activation_temp = del_cost_activation.copy()\n",
    "            del_cost_activation = np.squeeze(np.matmul(del_cost_activation_temp.reshape((1,-1)),self.weights_matrix[-1*i+1]))\n",
    "            del_cost_activation = np.where(z_layers[-1*i]>=0, del_cost_activation, 0)\n",
    "            image_delta_w[-1*i] = np.matmul(del_cost_activation.reshape((-1,1)), activation_layers[-1*i-1].reshape((1,-1)))\n",
    "            image_delta_b[-1*i] = del_cost_activation.copy()\n",
    "        \n",
    "        return image_delta_w, image_delta_b\n",
    "    \n",
    "    def train(self,train_data=[],number_of_epochs=5,learning_rate=0.01,batch_size=50,test_data=[],save_path = None):\n",
    "        data = train_data.copy()\n",
    "        previous_loss = float('Inf')\n",
    "        train_continue = True\n",
    "        early_stopping = np.zeros(3)\n",
    "        count = 0\n",
    "#         for epoch in range(number_of_epochs):\n",
    "        while(train_continue==True):\n",
    "            correct_prediction = 0\n",
    "            training_loss = 0\n",
    "            np.random.shuffle(data)\n",
    "            \n",
    "            for image in data:\n",
    "                output_softmax = self.fc_layer(image[:3072])\n",
    "                training_loss += self.cross_entropy(output_softmax,image[-1])\n",
    "                output = np.argmax(output_softmax)\n",
    "                if output == image[-1]:\n",
    "                    correct_prediction += 1\n",
    "                    \n",
    "                Training_Accuracy = (correct_prediction/data.shape[0])*100\n",
    "            \n",
    "            if test_data!=[]:\n",
    "                correct_prediction = 0\n",
    "                testing_loss = 0\n",
    "                for image in test_data:\n",
    "                    output_softmax = self.fc_layer(image[:3072])\n",
    "                    testing_loss += self.cross_entropy(output_softmax,image[-1])\n",
    "                    output = np.argmax(output_softmax)\n",
    "                    if output == image[-1]:\n",
    "                        correct_prediction += 1\n",
    "                Testing_Accuracy = (correct_prediction/test_data.shape[0])*100\n",
    "                \n",
    "                if testing_loss<previous_loss:\n",
    "                    previous_loss = testing_loss\n",
    "                    with open(save_path+'model_weight.pkl','wb') as f:\n",
    "                        pickle.dump(self.weights_matrix, f)\n",
    "                    with open(save_path+'model_bais.pkl','wb') as b:\n",
    "                        pickle.dump(self.bias_matrix, b)\n",
    "                \n",
    "                if (testing_loss-previous_loss)>=0.1:\n",
    "                    early_stopping[count%3] = 1\n",
    "                else:\n",
    "                    early_stopping[count%3] = 0\n",
    "                \n",
    "                if np.sum(early_stopping) == 3:\n",
    "                    train_continue = False\n",
    "                    \n",
    "                count+=1\n",
    "                \n",
    "#                 print('epoch:',count ,', Train Loss:', round(training_loss,3),', Test Loss:', round(testing_loss,3))\n",
    "                print('epoch:',count ,', Train Loss:', round(training_loss,3),', Test Loss:', round(testing_loss,3), ', Train Accuracy:', round(Training_Accuracy,3),'%',', Test Accuracy:', round(Testing_Accuracy,3),'%')\n",
    "            \n",
    "            else:\n",
    "                print('epoch:',epoch ,', Train Loss:', round(training_loss,3))\n",
    "                #print('epoch:',epoch ,', Train Loss:', round(training_loss,3),', Train Accuracy:', round(Training_Accuracy,3),'%')\n",
    "    \n",
    "            for batch_number in range(data.shape[0]//batch_size):\n",
    "                \n",
    "                batch_delta_w = [np.zeros((neuron_in_next_layers, neuron_in_previous_layers))\\\n",
    "                                 for neuron_in_previous_layers, neuron_in_next_layers in zip(self.architecture[:-1], self.architecture[1:])]\n",
    "                batch_delta_b = [np.zeros(number_of_neuron) for number_of_neuron in self.architecture[1:]]\n",
    "                \n",
    "                for image in data[batch_size*batch_number:batch_size*(batch_number+1)]:\n",
    "                    image_delta_w, image_delta_b = self.backpropagation(image[:3072], image[-1])\n",
    "                \n",
    "                    batch_delta_w = [batch_weight_delta+image_weight_delta for batch_weight_delta, image_weight_delta in zip(batch_delta_w, image_delta_w)]\n",
    "                    batch_delta_b = [batch_bais_delta+image_bais_delta for batch_bais_delta, image_bais_delta in zip(batch_delta_b, image_delta_b)]\n",
    "                    \n",
    "                \n",
    "                self.weights_matrix = [weight_layer-(learning_rate/batch_size)*weight_layer_delta\n",
    "                            for weight_layer, weight_layer_delta in zip(self.weights_matrix, batch_delta_w)]\n",
    "                self.bias_matrix = [bais_layer-(learning_rate/batch_size)*bais_layer_delta\n",
    "                           for bais_layer, bais_layer_delta in zip(self.bias_matrix, batch_delta_b)]\n",
    "            \n",
    "                del batch_delta_w, batch_delta_b, image_delta_w, image_delta_b\n",
    "    \n",
    "    def evaluate(self,test_data):\n",
    "        data = test_data.copy()\n",
    "        testing_loss = 0\n",
    "        correct_prediction = 0\n",
    "        for image in data:\n",
    "            output_softmax = self.fc_layer(image[:3072])\n",
    "            testing_loss += self.cross_entropy(output_softmax,image[-1])\n",
    "            output = np.argmax(output_softmax)\n",
    "            if output == image[-1]:\n",
    "                correct_prediction += 1\n",
    "        print('Test Loss:', round(testing_loss,3),', Test Accuracy:', round((correct_prediction/data.shape[0])*100,3),'%')\n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:101: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 , Train Loss: 7984.343 , Test Loss: 689.154 , Train Accuracy: 53.83 % , Test Accuracy: 55.55 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-925816fa9ab0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmy_network\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3072\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmy_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcifar_2_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcifar_2_data_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./model/2_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# train_data=cifar_2_data, number_of_epochs=100, learning_rate=0.0001, batch_size=10,test_data=cifar_2_data_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-fc42df48ec94>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_data, number_of_epochs, learning_rate, batch_size, test_data, save_path)\u001b[0m\n\u001b[0;32m    142\u001b[0m                     \u001b[0mimage_delta_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_delta_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackpropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3072\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                     \u001b[0mbatch_delta_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_weight_delta\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimage_weight_delta\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbatch_weight_delta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_weight_delta\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_delta_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_delta_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m                     \u001b[0mbatch_delta_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_bais_delta\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimage_bais_delta\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbatch_bais_delta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_bais_delta\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_delta_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_delta_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-fc42df48ec94>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    142\u001b[0m                     \u001b[0mimage_delta_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_delta_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackpropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3072\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                     \u001b[0mbatch_delta_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_weight_delta\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimage_weight_delta\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbatch_weight_delta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_weight_delta\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_delta_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_delta_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m                     \u001b[0mbatch_delta_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_bais_delta\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimage_bais_delta\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbatch_bais_delta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_bais_delta\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_delta_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_delta_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "my_network = neural_network([3072,100,2])\n",
    "my_network.train(train_data=cifar_2_data, number_of_epochs=100, learning_rate=0.0001, batch_size=10,test_data=cifar_2_data_test,save_path='./model/2_')\n",
    "# train_data=cifar_2_data, number_of_epochs=100, learning_rate=0.0001, batch_size=10,test_data=cifar_2_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 689.154 , Test Accuracy: 55.55 %\n"
     ]
    }
   ],
   "source": [
    "test_network = neural_network([3072,10,2],pretrained=True,model_path='./model/2_')\n",
    "test_network.evaluate(test_data=cifar_2_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:101: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 , Train Loss: 12748.378 , Test Loss: 3435.886 , Train Accuracy: 19.805 % , Test Accuracy: 17.1 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f7c131d2fbe0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmy_network_5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3072\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmy_network_5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcifar_5_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcifar_5_data_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./model/5_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# train_data=cifar_5_data, number_of_epochs=100, learning_rate=0.00001, batch_size=10,test_data=cifar_5_data_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-fc42df48ec94>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_data, number_of_epochs, learning_rate, batch_size, test_data, save_path)\u001b[0m\n\u001b[0;32m    142\u001b[0m                     \u001b[0mimage_delta_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_delta_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackpropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3072\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                     \u001b[0mbatch_delta_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_weight_delta\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimage_weight_delta\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbatch_weight_delta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_weight_delta\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_delta_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_delta_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m                     \u001b[0mbatch_delta_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_bais_delta\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimage_bais_delta\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbatch_bais_delta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_bais_delta\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_delta_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_delta_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-fc42df48ec94>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    142\u001b[0m                     \u001b[0mimage_delta_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_delta_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackpropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3072\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                     \u001b[0mbatch_delta_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_weight_delta\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimage_weight_delta\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbatch_weight_delta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_weight_delta\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_delta_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_delta_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m                     \u001b[0mbatch_delta_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_bais_delta\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimage_bais_delta\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbatch_bais_delta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_bais_delta\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_delta_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_delta_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "my_network_5 = neural_network([3072,50,5])\n",
    "my_network_5.train(train_data=cifar_5_data, number_of_epochs=100, learning_rate=0.00001, batch_size=10,test_data=cifar_5_data_test,save_path='./model/5_')\n",
    "# train_data=cifar_5_data, number_of_epochs=100, learning_rate=0.00001, batch_size=10,test_data=cifar_5_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3435.886 , Test Accuracy: 17.1 %\n"
     ]
    }
   ],
   "source": [
    "test_network_5 = neural_network([3072,10,5],pretrained=True,model_path='./model/5_')\n",
    "test_network_5.evaluate(test_data=cifar_5_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
